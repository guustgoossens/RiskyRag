# RiskyRag (RiskyRagBench)

**Multiplayer grand strategy game where LLMs play historically accurate Risk**

[![NexHacks 2026](https://img.shields.io/badge/NexHacks-2026-blue)](https://nexhacks.com)
[![Education](https://img.shields.io/badge/Track-Education-green)](https://nexhacks.com)
[![DevTools](https://img.shields.io/badge/Track-DevTools%20%26%20Infrastructure-orange)](https://nexhacks.com)

## What is this?

RiskyRag is a "Risk"-style strategy game where humans compete against LLM-powered AI agents. The twist: **temporal knowledge filtering**. Choose a start date (1453, 1776, 1914), and AI agents only know history up to that point.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "It's 1453. You are the Ottoman Empire" â”‚
â”‚  ğŸ¤– AI: "I will besiege Constantinople"  â”‚
â”‚                                           â”‚
â”‚  Historical RAG filtered to â‰¤1453        â”‚
â”‚  No "future knowledge" leakage           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Matters

1. **Novel LLM Benchmark:** Tests strategic reasoning, tool usage, and temporal constraint adherence
2. **Educational Platform:** Learn history by playing as/against historically accurate AI
3. **Technical Innovation:** First implementation of temporally-filtered RAG using Apple's CLaRa
4. **Reusable Infrastructure:** Open-source temporal RAG that works for any time-sensitive domain

## Quick Start

```bash
# Clone repo
cd /Users/guustgoossens/Desktop/Accaio/RiskyRag

# Install dependencies
bun install                  # Frontend + backend
cd scraper && uv sync       # Python scraping pipeline

# Set up Convex
npx convex dev

# Run scrapers (pre-populate historical data)
uv run riskyrag scrape wikipedia_events --date-range 1400-1500
uv run riskyrag embed --model voyage-2
uv run riskyrag sync-to-convex

# Start vLLM server (requires GPU or use OpenAI fallback)
# See vllm-setup.md for details

# Start frontend
bun dev
```

Open [http://localhost:5173](http://localhost:5173) to play!

## Architecture

```
Frontend (Vite + React)
    â†“ WebSocket
Convex Backend (Real-time DB)
    â†“ MCP Tools
vLLM Server (Llama 3.2 7B)
    â†“ RAG Query
Temporal RAG (CLaRa + Voyage)
    â†“ Filtered by date
Historical Snippets Database
```

## Key Features

- â° **Temporal RAG:** AI only knows history up to game date
- ğŸ® **Real-time Multiplayer:** Powered by Convex
- ğŸ¤– **LLM Agents:** Play against GPT-4, Claude, or Llama
- ğŸ“š **Educational Mode:** Ask AI about historical events
- ğŸ“Š **Benchmarking:** Compare LLM strategic reasoning
- ğŸŒ **Historical Scenarios:** 1453, 1776, 1914, and more

## Tech Stack

| Layer | Technology |
|-------|------------|
| Frontend | Vite + React + TanStack Router |
| Backend | Convex (real-time database) |
| LLM Inference | vLLM + Llama 3.2 7B/13B |
| RAG | Apple CLaRa + Voyage embeddings |
| Scraping | Partner's extraction engine + Python |
| Package Managers | bun (TS), uv (Python) |

## Project Structure

```
RiskyRag/
â”œâ”€â”€ PRD.md                    # Full product requirements
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ CLAUDE.md                 # AI assistant instructions
â”‚
â”œâ”€â”€ frontend/                 # Game UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ routes/          # TanStack Router pages
â”‚   â”‚   â”œâ”€â”€ components/      # React components
â”‚   â”‚   â””â”€â”€ lib/             # Utilities
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ convex/                   # Backend (Convex)
â”‚   â”œâ”€â”€ schema.ts            # Database schema
â”‚   â”œâ”€â”€ game.ts              # Game logic mutations
â”‚   â”œâ”€â”€ rag.ts               # RAG queries
â”‚   â””â”€â”€ agent.ts             # LLM agent interface
â”‚
â”œâ”€â”€ scraper/                  # Historical data pipeline
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ scrapers/        # Wikipedia, etc.
â”‚   â”‚   â”œâ”€â”€ ingest.py        # Embedding generation
â”‚   â”‚   â””â”€â”€ sync.py          # Upload to Convex
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ vllm-server/             # LLM inference
â”‚   â””â”€â”€ setup.md
â”‚
â””â”€â”€ benchmarks/              # Evaluation scripts
    â””â”€â”€ run_tournament.ts
```

## Core Concepts

### 1. Temporal RAG

**Problem:** LLMs know the future (from training data)

**Solution:** Filter RAG results by date

```python
def retrieve(query: str, max_date: datetime):
    """Only return docs where event_date <= max_date"""
    results = vector_search(
        embed(query),
        filters={"event_date": {"$lte": max_date}}
    )
    return results
```

### 2. LLM Agent Tools

AI agents interact via function calling:

- `get_game_state()` - See map, territories, troops
- `attack_territory(from, to, troops)` - Launch attack
- `move_troops(from, to, count)` - Reposition
- `send_negotiation(recipient, msg)` - Diplomacy
- `query_history(question)` - Learn about the past

### 3. Historical Scenarios

| Year | Event | Nations |
|------|-------|---------|
| 1453 | Fall of Constantinople | Ottomans, Byzantines, Venice |
| 1776 | American Revolution | Britain, France, Colonies |
| 1914 | WWI Start | Germany, France, Britain, Russia |

## Development Roadmap

**Phase 1: Infrastructure (Hours 0-8)**
- Set up Convex + vLLM
- Basic frontend scaffold
- Python scraping environment

**Phase 2: Core Game (Hours 8-20)**
- Game state management
- Attack/move/fortify mechanics
- Map UI for 1453 scenario

**Phase 3: Temporal RAG (Hours 20-32)** â­
- Scrape Wikipedia (1400-1500)
- Implement date filtering
- Integrate with LLM agents

**Phase 4: Polish (Hours 32-48)**
- Negotiation UI
- Historical query feature
- Benchmarking metrics
- Demo preparation

## Contributing

We're building this at **NexHacks 2026** (Jan 18-19, Pittsburgh). Team:
- **Backend Lead:** [@rolimups](https://github.com/yourusername)
- **Data Lead:** [@partner](https://github.com/partner) (scraping + CLaRa)
- **Frontend Lead:** TBD

Want to join? DM us on Discord!

## Benchmarking

Run automated tournaments to compare LLMs:

```bash
bun run benchmark \
  --models gpt-4,llama-3.2-7b,claude-sonnet-3.5 \
  --scenario 1453 \
  --games 50
```

Metrics tracked:
- Win rate
- Average game length
- Tool usage patterns
- Historical accuracy score

## License

MIT (post-hackathon)

## Acknowledgments

- **Apple CLaRa** for temporal RAG inspiration
- **vLLM** for efficient LLM inference
- **Convex** for real-time backend magic
- **NexHacks** for the opportunity

---

**Built with ğŸ”¥ for NexHacks 2026**

*Where history meets AI meets strategy*
